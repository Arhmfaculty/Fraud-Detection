# -*- coding: utf-8 -*-
"""Maxwell_Fraud Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R8ZoKjWS5l_tfBooTqLPv2e6_86DzwDp
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from imblearn.over_sampling import SMOTE
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import cross_val_score
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
import joblib

data = pd.read_csv('fraud_detection.xls')
data.head()

data.shape

data.info()

#change Is_Fraud from float to int
data['Is_Fraud'] = data['Is_Fraud'].astype(int)

data.describe()

#Check for duplicates in the data set
dup=data.duplicated()
print(dup)

#handle missing values
data.isnull().sum()

# checking for unique values
data.nunique()

#visualise outliers for numerical data
sns.boxplot(data=data)

# Encode categorical to numerical
le = LabelEncoder()
data['Previous_Location'] = le.fit_transform(data['Previous_Location'])
data['New_Location'] = le.fit_transform(data['New_Location'])
data.head()

#use Location_Change (True/False) as an indicator feature.
data['Location_Change'] = np.where(data['Previous_Location'] == data['New_Location'], 0, 1)
data.head()



# Plotting Senior Location Change Proportion

senior_citizen_counts = data['Location_Change'].value_counts()
labels = ['Yes', 'No']
colors = ['#008fd5', '#fc4f30']
explode = (0, 0.1)
plt.pie(senior_citizen_counts, labels=labels, explode=explode, colors=colors,
        autopct='%1.1f%%', shadow=True, startangle=90)
plt.axis('equal')
plt.title('Location Change Proportion')
plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1))
plt.ylabel('')
plt.show()

"""A higer proportion of the people doing the trasaction changed their location with close to 90% of the dataset showing change in location."""

# Plotting Senior Is_Fraud Proportion

senior_citizen_counts = data['Is_Fraud'].value_counts()
labels = ['No', 'Yes']
colors = ['#008fd5', '#fc4f30']
explode = (0, 0.1)
plt.pie(senior_citizen_counts, labels=labels, explode=explode, colors=colors,
        autopct='%1.1f%%', shadow=True, startangle=90)
plt.axis('equal')
plt.title('Is_Fraud Proportion')
plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1))
plt.ylabel('')
plt.show()

"""Only 2% of the dataset is identified as fraud. While it is not significant, it is still show a higher number of the data as fraud which needs to be further explored"""

#show a normal distribution of the Transaction time
sns.distplot(data['Transaction_Time'])

# display transaction Time effect on fraud
data.groupby('Is_Fraud')['Transaction_Time'].mean()

data.corr()

#heatmap
plt.figure(figsize=(10,10))
sns.heatmap(data.corr(), annot=True)

"""The heatmap shows that how Transaction amount closely correlate positively with the Is Fraud. Also, Acount age shows a bit of negative correlation"""

# Compute the correlation matrix
corr_matrix = data.corr()

# Extract the correlation of all variables with the 'Churn' variable
churn_corr = corr_matrix['Is_Fraud'].sort_values(ascending=False)

# Plotting the correlation of all features with 'Churn'
plt.figure(figsize=(10, 8))
sns.barplot(x=churn_corr.index, y=churn_corr.values, palette='coolwarm')
plt.xticks(rotation=90)
plt.title('Correlation of Features with Is_Fraud')
plt.xlabel('Features')
plt.ylabel('Correlation with Is_Fraud')
plt.tight_layout()
plt.show()

"""Because there is closer to only 2% of fraud detected, the correlation matrix won't be giving enough information whether it affects fraud or not. However, from research, the New location and old location won't necessariry impact the fraud. So I will check a change in location and drop these two features"""

#Drop the 'New_Location' and 'Previous_Location' columns
data.drop(['New_Location', 'Previous_Location'], axis=1, inplace=True)
data.head()

"""#MODELLING"""

# Plot the distribution of is fraud values
plt.figure(figsize=(8, 6))
sns.countplot(x='Is_Fraud', data=data, palette='viridis')
plt.title('Distribution of Is_Fraud Values')
plt.xlabel('Fruad')
plt.ylabel('Count')
plt.show()

"""Given the significant imbalance between the Is_Fraud values, I applied SMOTE to increase the representation of the minority class (Fraud) in the dataset."""

#Spliting the data
X = data.drop('Is_Fraud', axis=1)
y = data['Is_Fraud']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply SMOTE to the training data to balance it
smote = SMOTE(random_state=42)
X_train_feature, y_train_feature = smote.fit_resample(X_train, y_train)

# Display new distribution with fraud
plt.figure(figsize=(8, 6))
sns.countplot(x=y_train_feature, palette='viridis')
plt.title('Distribution of Is_Fraud Values')
plt.xlabel('Fruad')
plt.ylabel('Count')

"""#NAIVE BAYES MODEL"""

# Initialize model
model = GaussianNB()

# Train the model
model.fit(X_train_feature, y_train_feature)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Cross-validation
cv_scores = cross_val_score(model, X, y, cv=5)

# Display results
print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1:.2f}")
print(f"Cross-Validation Scores: {cv_scores}")
print(f"Mean CV Score: {cv_scores.mean():.2f}")

"""The Naive Bayes model demonstrated strong performance, especially in detecting fraudulent transactions (100% recall). However, the lower precision of 73% indicates that the model might produce some false positives. This could be acceptable in fraud detection scenarios where missing a fraudulent transaction (false negative) is riskier than mistakenly flagging a legitimate transaction (false positive).

Additionally, the high cross-validation scores (with an average of 1.00) show that the model performs consistently across different subsets of the dataset. This suggests that the model generalizes well and is unlikely to suffer from overfitting, making it a reliable choice for fraud detection in this context

# USING GDA MODEL
"""

# Initialize the GDA model (covariance type 'auto' for Gaussian)
gda = LDA()

# Train the model
gda.fit(X_train_feature, y_train_feature)

# Make predictions on the test set
y_pred = gda.predict(X_test)

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

# Print evaluation results
print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1:.2f}")
print("Confusion Matrix:")
print(cm)

# Perform 5-fold cross-validation
cv_scores = cross_val_score(gda, X, y, cv=5, scoring='accuracy')

# Print the cross-validation scores
print(f"Cross-Validation Scores: {cv_scores}")
print(f"Mean Cross-Validation Accuracy: {np.mean(cv_scores):.2f}")

"""The GDA model exhibited excellent performance, with perfect precision and high recall, making it a strong candidate for fraud detection. The absence of false positives is a significant advantage in real-world applications, where mistakenly flagging legitimate transactions can cause customer dissatisfaction. The slightly lower recall indicates a small number of missed fraudulent transactions, which could be improved with further tuning. The high cross-validation accuracy demonstrates the model's robustness and suggests it can generalize well to new, unseen data

#Comparing the models
The GDA model is the preferred choice for this fraud detection task. It offers a better balance between high precision and recall, minimizing both false positives and false negatives. Its robust performance across cross-validation also indicates strong generalization, making it a reliable model for predicting fraudulent transactions in this dataset.

# Creating a program to test the algorithm
"""

joblib.dump(gda, "gda_model.pkl")

# Load the trained GDA model
model = joblib.load("gda_model.pkl")

# Ask user for input features
transaction_amount = float(input("Enter Transaction Amount: "))
transaction_time = float(input("Enter Transaction Time (e.g., hour of the day): "))
account_age = float(input("Enter Account Age (in years): "))
location_change = int(input("Enter Location Change (1 for change, 0 for no change): "))

# Prepare input features as an array
features = np.array([[transaction_amount, transaction_time, account_age, location_change]])

# Make prediction
prediction = model.predict(features)

# Output result
if prediction == 1:
    print("⚠️ The transaction is predicted to be FRAUDULENT.")
else:
    print("✅ The transaction is predicted to be LEGITIMATE.")

"""#Analysis

The key features influencing fraud detection in the model are **Transaction Amount, Transaction Time, Account Age, and Location Change**. Among these, **Transaction Amount and Location Change** are likely the most impactful. Large transaction amounts and sudden changes in location are common indicators of fraudulent activity, making these features particularly useful in detecting fraud. While Transaction Time and Account Age also contribute, they may play a less direct role in identifying fraud compared to the former two. However, upon testing with new values out the given dataset, it was also found that, extremely larger Account Age also impact the legitimacy of the transaction.

The perfomance might be influenced by the **overfitting**, as the model achieves near-perfect accuracy, which suggests that it may be too closely tuned to the training data and not generalizing well to unseen data.

To address these issues,
* Feature engineering can be enhanced by incorporating additional user behavioral patterns and geographic data. These extra features can help capture more nuanced information about transaction behavior and improve the model's ability to distinguish between legitimate and fraudulent transactions.
* Using more advanced models, such as ensemble methods like Random Forest or Gradient Boosting, could help improve the model's generalization and reduce overfitting.
* Regular retraining with new data and fine-tuning hyperparameters would also be beneficial for maintaining performance
"""